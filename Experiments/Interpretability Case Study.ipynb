{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import time\n",
    "from numba import jit\n",
    "import itertools\n",
    "import random   \n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "def get_node_depths(tree1):\n",
    "    \"\"\"\n",
    "    Get the node depths of the decision tree\n",
    "\n",
    "    >>> d = DecisionTreeClassifier()\n",
    "    >>> d.fit([[1,2,3],[4,5,6],[7,8,9]], [1,2,3])\n",
    "    >>> get_node_depths(d.tree_)\n",
    "    array([0, 1, 1, 2, 2])\n",
    "    \"\"\"\n",
    "    def get_node_depths_(current_node, current_depth, l, r, depths):\n",
    "        depths += [current_depth]\n",
    "        if l[current_node] != -1 and r[current_node] != -1:\n",
    "            get_node_depths_(l[current_node], current_depth + 1, l, r, depths)\n",
    "            get_node_depths_(r[current_node], current_depth + 1, l, r, depths)\n",
    "\n",
    "    depths = []\n",
    "    get_node_depths_(0, 0, tree1.tree_.children_left, tree1.tree_.children_right, depths) \n",
    "    return np.array(depths)\n",
    "\n",
    "def get_node_count(tree_list,best_vars):\n",
    "    num_nodes = 0\n",
    "    depths = np.sum(best_vars,axis = 1)\n",
    "    for i in range(len(best_vars)):\n",
    "        tree1 = tree_list[i]\n",
    "        node_depths = get_node_depths(tree1)\n",
    "        depth_cutoff = depths[i]\n",
    "        if depth_cutoff > 0:\n",
    "            num_nodes = num_nodes + sum(node_depths <= depth_cutoff)\n",
    "    return num_nodes\n",
    "\n",
    "\n",
    "def difference_array_list(X,tree_list):\n",
    "    diff_array_list = []\n",
    "    for tree1 in tree_list:\n",
    "        diff_array_list.append(difference_array(X,tree1))\n",
    "    return np.array(diff_array_list)\n",
    "\n",
    "def difference_array(X, tree_learner):\n",
    "    \"\"\"function that takes a decision tree and returns an \n",
    "    [m,d]\n",
    "    Each row is an instance and each column is a depth level.\n",
    "    We take the difference in internal node values to get the delta for each depth level.\n",
    "    the column sum of the output is the prediction of the tree\n",
    "    \"\"\"\n",
    "    \n",
    "    node_indicator = tree_learner.decision_path(X)\n",
    "    values = tree_learner.tree_.value\n",
    "    vdiffs = []\n",
    "    \n",
    "    for i in range(0,len(X)):\n",
    "        node_ids = node_indicator.indices[node_indicator.indptr[i] : node_indicator.indptr[i + 1]]\n",
    "        instance_values = np.ndarray.flatten(values[node_ids])\n",
    "        diffs = [j-i for i, j in zip(instance_values[:-1], instance_values[1:])]\n",
    "        row = np.zeros(tree_learner.max_depth)\n",
    "        row[:len(diffs)] = diffs\n",
    "        vdiffs.append(row)\n",
    "        \n",
    "    return np.array(vdiffs)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluate_test_error(difference_array_list,Y,vars_z,learning_rate):\n",
    "    pred = np.zeros(len(Y))\n",
    "    for i in range(len(vars_z)):\n",
    "        pred += np.dot(difference_array_list[i],vars_z[i])*learning_rate     \n",
    "    return np.square(np.subtract(Y, pred)).mean()\n",
    "\n",
    "from numba import jit\n",
    "import itertools\n",
    "import random    \n",
    "@jit(nopython=True)\n",
    "def precompute_predictions(diff_array_list,temp_vars,learning_rate,cycle_ind):\n",
    "    \n",
    "    precompute_pred = np.zeros(len(diff_array_list[0]))    \n",
    "    for i in range(len(diff_array_list)):\n",
    "        if i != cycle_ind:\n",
    "            precompute_pred += np.dot(diff_array_list[i],temp_vars[i])*learning_rate \n",
    "   \n",
    "    return precompute_pred\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluate_candidates(diff_array_list,temp_vars,learning_rate,cycle_ind,candidates,\n",
    "                        precompute_pred,Y,alpha,W_array, normalization):\n",
    "    scores = []\n",
    "    for candidate in candidates:\n",
    "        temp_vars[cycle_ind] = candidate\n",
    "        pred_candidate = np.dot(diff_array_list[cycle_ind],candidate)*learning_rate\n",
    "        pred = np.add(precompute_pred,pred_candidate)\n",
    "        err = np.sum((Y-pred)**2)/len(Y) + (alpha/normalization)*np.sum(np.dot(W_array[cycle_ind],candidate))\n",
    "        scores.append(err)\n",
    "    return scores\n",
    "\n",
    "@jit(nopython=True)\n",
    "def eval_obj(Y,diff_array_list,vars_z,learning_rate,alpha,W_array,normalization):\n",
    "    pred = np.zeros(len(Y))\n",
    "    regularization = 0\n",
    "    for i in range(len(vars_z)):\n",
    "        pred+= learning_rate*np.dot(diff_array_list[i],vars_z[i])\n",
    "        regularization += np.sum(np.dot(W_array[i],vars_z[i]))\n",
    "    \n",
    "    bias = np.sum((Y-pred)**2)/len(Y)\n",
    "    \n",
    "    return bias + regularization*alpha/normalization\n",
    "\n",
    "@jit(nopython=True)\n",
    "def converge_test(sequence, threshold,tail_length):\n",
    "    diff = np.diff(sequence)\n",
    "    if len(diff) < (tail_length+1):\n",
    "        return False\n",
    "    else:\n",
    "        return (np.max(np.abs(diff[-tail_length:])) < threshold)\n",
    "\n",
    "\n",
    "def solve_weighted(Y,tree_list,diff_array_list,alpha,learning_rate,\n",
    "                                          W_array,normalization,warm_start= []):\n",
    "    max_depth = tree_list[0].max_depth\n",
    "    Y = np.array(Y.values)\n",
    "    \n",
    "    vars_z = np.zeros((len(tree_list),max_depth))\n",
    "    if len(warm_start) > 0:\n",
    "        vars_z = np.array(warm_start)\n",
    "    \n",
    "    candidates = np.vstack([np.zeros(max_depth),np.tril(np.ones((max_depth,max_depth)))])\n",
    "    \n",
    "    convergence_scores = np.array([])\n",
    "    converged = False\n",
    "    ind_counter = 0\n",
    "    local_best = 9999\n",
    "    total_inds = 0\n",
    "    while converged == False:\n",
    "        \n",
    "        cycle_ind = ind_counter % len(vars_z)   \n",
    "\n",
    "        temp_vars= vars_z.copy()\n",
    "        precompute_pred = precompute_predictions(diff_array_list,temp_vars,learning_rate,cycle_ind)\n",
    "        scores = evaluate_candidates(diff_array_list,temp_vars,learning_rate,cycle_ind,\n",
    "                                     candidates,precompute_pred,Y,alpha,W_array,normalization)\n",
    "        \n",
    "        vars_z[cycle_ind] = candidates[np.argmin(scores)]\n",
    "        convergence_scores = np.append(convergence_scores,eval_obj(Y,diff_array_list,\n",
    "                                                                   vars_z,learning_rate,alpha,W_array,normalization))\n",
    "        converged = converge_test(np.array(convergence_scores),10**-6,3)\n",
    "        \n",
    "        ind_counter = ind_counter + 1\n",
    "        total_inds = total_inds + 1\n",
    "        \n",
    "        #local search\n",
    "        if converged == True:\n",
    "            support_indicies = np.where(~np.all(vars_z == 0, axis=1))[0]\n",
    "            zero_indicies = np.where(np.all(vars_z == 0, axis=1))[0]\n",
    "            \n",
    "            if convergence_scores[-1] > local_best:\n",
    "                converged = True\n",
    "            \n",
    "            elif len(support_indicies)> 0:\n",
    "                local_ind = random.choice(support_indicies)\n",
    "                vars_z[local_ind] = np.zeros(max_depth)\n",
    "                \n",
    "                if len(zero_indicies) > 0:\n",
    "                    ind_counter = min(zero_indicies)\n",
    "                    converged = False\n",
    "                    local_best = convergence_scores[-1]\n",
    "                \n",
    "                else:\n",
    "                    converged = True\n",
    "     \n",
    "    return vars_z , total_inds\n",
    "\n",
    "# Weight Penalties\n",
    "\n",
    "def nodes_per_layer(tree_list):\n",
    "    max_depth = tree_list[0].max_depth\n",
    "    results = []\n",
    "    for tree1 in tree_list:\n",
    "        depths = get_node_depths(tree1)\n",
    "        values,counts = np.unique(depths,return_counts = True)\n",
    "        diag = np.zeros(max_depth)\n",
    "        counts = counts[1:]\n",
    "        diag[:len(counts)] = counts\n",
    "        results.append(np.diag(diag))\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "def total_nodes(tree_list):\n",
    "    return np.sum(tree1.tree_.node_count for tree1 in tree_list) - len(tree_list)\n",
    "\n",
    "def prune_polish(difference_array_list,Y,vars_z,learning_rate):\n",
    "    pred_array = []\n",
    "    for i in range(len(vars_z)):\n",
    "        if sum(vars_z[i])>0:\n",
    "            pred_array.append(np.dot(difference_array_list[i],vars_z[i])*learning_rate)\n",
    "    \n",
    "    if len(pred_array) == 0:\n",
    "        return np.zeros(len(vars_z))\n",
    "    \n",
    "    pred_array = np.transpose(pred_array)\n",
    "    lm = Ridge(alpha = 0.01, fit_intercept = False).fit(pred_array,Y)\n",
    "    coef = lm.coef_\n",
    "    return coef\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluate_test_error_polished(difference_array_list,Y,vars_z,coef,learning_rate):\n",
    "    pred = np.zeros(len(Y))\n",
    "    j = 0\n",
    "    for i in range(len(vars_z)):\n",
    "        if sum(vars_z[i])>0:\n",
    "            pred += np.dot(difference_array_list[i],vars_z[i])*learning_rate*coef[j]  \n",
    "            j+=1\n",
    "    return np.square(np.subtract(Y, pred)).mean(), pred\n",
    "\n",
    "import time\n",
    "def subensemble_predict(X,tree_list,learning_rate,ntrees):\n",
    "    pred = np.zeros(len(X))\n",
    "    for tree1 in tree_list[:ntrees]:\n",
    "        pred += tree1.predict(X)*learning_rate\n",
    "    return pred\n",
    "\n",
    "def get_node_count_all(tree_list):\n",
    "    num_nodes = 0\n",
    "    for tree1 in tree_list:\n",
    "        num_nodes = num_nodes + tree1.tree_.node_count\n",
    "    return num_nodes\n",
    "\n",
    "def lasso_predict(X,tree_list,coef):\n",
    "    pred = np.zeros(len(X))\n",
    "    for i in range(len(tree_list)):\n",
    "        pred += tree_list[i].predict(X)*coef[i]\n",
    "    return pred\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from itertools import product\n",
    "\n",
    "def miqp(features, response, non_zero, warm_up=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Deploy and optimize the MIQP formulation of L0-Regression.\n",
    "    \"\"\"\n",
    "    assert isinstance(non_zero, (int, np.integer))\n",
    "    regressor = gp.Model()\n",
    "    samples, dim = features.shape\n",
    "    assert samples == response.shape[0]\n",
    "    assert non_zero <= dim\n",
    "\n",
    "    # Append a column of ones to the feature matrix to account for the y-intercept\n",
    "    X = features\n",
    "    \n",
    "    # Decision variables\n",
    "    beta = regressor.addVars(dim, lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
    "\n",
    "    # iszero[i] = 1 if beta[i] = 0  \n",
    "    iszero = regressor.addVars(dim, vtype=GRB.BINARY, name=\"iszero\") \n",
    "    \n",
    "    # Objective Function (OF): minimize 1/2 * RSS using the fact that\n",
    "    # if x* is a minimizer of f(x), it is also a minimizer of k*f(x) iff k > 0\n",
    "    Quad = np.dot(X.T, X)\n",
    "    lin = np.dot(response.T, X)\n",
    "    obj = sum(0.5 * Quad[i,j] * beta[i] * beta[j]\n",
    "              for i, j in product(range(dim ), repeat=2))\n",
    "    obj -= sum(lin[i] * beta[i] for i in range(dim))\n",
    "    obj += 0.5 * np.dot(response, response)\n",
    "    regressor.setObjective(obj, GRB.MINIMIZE)\n",
    "    \n",
    "    # Constraint sets\n",
    "    for i in range(dim):\n",
    "        # If iszero[i]=1, then beta[i] = 0\n",
    "        regressor.addSOS(GRB.SOS_TYPE1, [beta[i], iszero[i]])\n",
    "    regressor.addConstr(iszero.sum() == dim - non_zero) # Budget constraint\n",
    "\n",
    "    # We may use the Lasso or prev solution with fewer features as warm start\n",
    "    if warm_up is not None and len(warm_up) == dim:\n",
    "        for i in range(dim):\n",
    "            iszero[i].start = (abs(warm_up[i]) < 1e-6)\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = 180\n",
    "    regressor.params.mipgap = 0.001\n",
    "    regressor.optimize()\n",
    "\n",
    "    coeff = np.array([beta[i].X for i in range(dim)])\n",
    "    return  coeff  \n",
    "\n",
    "\n",
    "def miqp_nneg(features, response, non_zero, warm_up=None, verbose=True,time_limit = 60):\n",
    "    \"\"\"\n",
    "    Deploy and optimize the MIQP formulation of L0-Regression.\n",
    "    \"\"\"\n",
    "    assert isinstance(non_zero, (int, np.integer))\n",
    "    regressor = gp.Model()\n",
    "    samples, dim = features.shape\n",
    "    assert samples == response.shape[0]\n",
    "    assert non_zero <= dim\n",
    "\n",
    "    # Append a column of ones to the feature matrix to account for the y-intercept\n",
    "    X = features\n",
    "\n",
    "    \n",
    "    # Decision variables\n",
    "    beta = regressor.addVars(dim, lb=0, name=\"beta\") # Weights\n",
    "\n",
    "    # iszero[i] = 1 if beta[i] = 0  \n",
    "    iszero = regressor.addVars(dim, vtype=GRB.BINARY, name=\"iszero\") \n",
    "    \n",
    "    # Objective Function (OF): minimize 1/2 * RSS using the fact that\n",
    "    # if x* is a minimizer of f(x), it is also a minimizer of k*f(x) iff k > 0\n",
    "    Quad = np.dot(X.T, X)\n",
    "    lin = np.dot(response.T, X)\n",
    "    obj = sum(0.5 * Quad[i,j] * beta[i] * beta[j]\n",
    "              for i, j in product(range(dim ), repeat=2))\n",
    "    obj -= sum(lin[i] * beta[i] for i in range(dim))\n",
    "    obj += 0.5 * np.dot(response, response)\n",
    "    regressor.setObjective(obj, GRB.MINIMIZE)\n",
    "    \n",
    "    # Constraint sets\n",
    "    for i in range(dim):\n",
    "        # If iszero[i]=1, then beta[i] = 0\n",
    "        regressor.addSOS(GRB.SOS_TYPE1, [beta[i], iszero[i]])\n",
    "    regressor.addConstr(iszero.sum() == dim - non_zero) # Budget constraint\n",
    "\n",
    "    # We may use the Lasso or prev solution with fewer features as warm start\n",
    "    if warm_up is not None and len(warm_up) == dim:\n",
    "        for i in range(dim):\n",
    "            iszero[i].start = (abs(warm_up[i]) < 1e-6)\n",
    "    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = time_limit\n",
    "    regressor.params.mipgap = 0.001\n",
    "    regressor.optimize()\n",
    "\n",
    "    coeff = np.array([beta[i].X for i in range(dim)])\n",
    "    return  coeff     \n",
    "\n",
    "\n",
    "\n",
    "def prune_polish_l0(difference_array_list,Y,vars_z,learning_rate, K, time_limit):\n",
    "    \n",
    "    pred_array = []\n",
    "    for i in range(len(vars_z)):\n",
    "        if sum(vars_z[i])>0:\n",
    "            pred_array.append(np.dot(difference_array_list[i],vars_z[i])*learning_rate)\n",
    "    \n",
    "    if len(pred_array) == 0:\n",
    "        return np.zeros(len(vars_z))\n",
    "    \n",
    "    pred_array = np.transpose(pred_array) \n",
    "    coef = miqp_nneg(pred_array,Y,K, time_limit = time_limit)\n",
    "    \n",
    "    return coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load data from  https://www2.census.gov/adrm/PDB/2019/ [pdb2019trv3_us]\n",
    "\"\"\"\n",
    "\n",
    "def load_data(load_directory='',\n",
    "              filename='pdb2019trv3_us.csv',\n",
    "              remove_margin_of_error_variables=False): \n",
    "    \"\"\"Loads Census data, and retrieves covariates and responses.\n",
    "    \n",
    "    Args:\n",
    "        load_directory: Data directory for loading Census file, str.\n",
    "        filename: file to load, default is 'pdb2019trv3_us.csv'.\n",
    "        remove_margin_of_error_variables: whether to remove margin of error variables, bool scaler.\n",
    "        \n",
    "    Returns:\n",
    "        df_X, covariates, pandas dataframe.\n",
    "        df_y, target response, pandas dataframe.\n",
    "    \"\"\"\n",
    "    file = os.path.join(load_directory, filename)\n",
    "    df = pd.read_csv(file, encoding = \"ISO-8859-1\")\n",
    "    df = df.set_index('GIDTR')\n",
    "    \n",
    "    # Drop location variables\n",
    "    drop_location_variables = ['State', 'State_name', 'County', 'County_name', 'Tract', 'Flag', 'AIAN_LAND']\n",
    "    df = df.drop(drop_location_variables, axis=1)\n",
    "    \n",
    "    target_response = 'Self_Response_Rate_ACS_13_17'\n",
    "    # Remove extra response variables \n",
    "    # Remove response columns 'FRST_FRMS_CEN_2010' (Number of addresses in a 2010 Census Mailout/Mailback area where the first form mailed was completed and returned) and 'RPLCMNT_FRMS_CEN_2010' (Number of addresses in a 2010 Census Mailout/Mailback area where the replacement form was completed and returned)\n",
    "\n",
    "    extra_response_variables = [\n",
    "        'Census_Mail_Returns_CEN_2010',\n",
    "        'Mail_Return_Rate_CEN_2010',\n",
    "        'pct_Census_Mail_Returns_CEN_2010',\n",
    "        'Low_Response_Score',\n",
    "        'Self_Response_Rate_ACSMOE_13_17',\n",
    "        'BILQ_Frms_CEN_2010',\n",
    "        'FRST_FRMS_CEN_2010',\n",
    "        'RPLCMNT_FRMS_CEN_2010',\n",
    "        'pct_FRST_FRMS_CEN_2010',\n",
    "        'pct_RPLCMNT_FRMS_CEN_2010']\n",
    "    df = df.drop(extra_response_variables, axis=1)\n",
    "    \n",
    "    if remove_margin_of_error_variables:\n",
    "        df = df[np.array([c for c in df.columns if 'MOE' not in c])]\n",
    "\n",
    "    # Change types of covariate columns with dollar signs in their values e.g. income, housing price  \n",
    "    df[df.select_dtypes('object').columns] = df[df.select_dtypes('object').columns].replace('[\\$,]', '', regex=True).astype(np.float64)\n",
    "\n",
    "    # Remove entries with missing predictions\n",
    "    df_full = df.copy()\n",
    "    df = df.dropna(subset=[target_response])\n",
    "\n",
    "    df_y = df[[target_response]]\n",
    "    df_X = df.drop([target_response], axis=1)\n",
    "\n",
    "\n",
    "    return df_X, df_y, df_full\n",
    "\n",
    "def process_data(df_X,\n",
    "                 df_y,\n",
    "                 val_ratio=0.1, \n",
    "                 test_ratio=0.1, \n",
    "                 seed=None,\n",
    "                 standardize_response=False):\n",
    "    \"\"\"Preprocesses covariates and response and generates training, validation and testing sets.\n",
    "    \n",
    "      Features are processed as follows:\n",
    "      Missing values are imputed using the mean. After imputation, all features are standardized. \n",
    "      Responses are processed as follow:\n",
    "      Either standardized or not depending on user choice selected by standardize_response.\n",
    "    Args:\n",
    "        val_ratio: Percentage of samples to be used for validation, float scalar.\n",
    "        test_ratio: Percentage of samples to be used for testing, float scalar.\n",
    "        seed: for reproducibility of results, int scalar.\n",
    "        standardize_response: whether to standardize target response or not, bool scalar.\n",
    "        \n",
    "    Returns:\n",
    "        X_train: Training processed covariates, float numpy array of shape (N, p).\n",
    "        y_train: Training (processed) responses, float numpy array of shape (N, ).\n",
    "        X_val: Validation processed covariates, float numpy array of shape (Nval, p).\n",
    "        y_val: Validation (processed) responses, float numpy array of shape (N, ).\n",
    "        X_test: Test processed covariates, float numpy array of shape (Ntest, p).\n",
    "        y_test: Test (processed) responses, float numpy array of shape (N, ).\n",
    "        x_preprocessor: processor for covariates, sklearn transformer.\n",
    "        y_preprocessor: processor for responses, sklearn transformer.\n",
    "    \"\"\"        \n",
    "        \n",
    "    N, p = df_X.shape\n",
    "    df_X_temp, df_X_test, df_y_temp, df_y_test = train_test_split(df_X, df_y, test_size=int(test_ratio*N), random_state=seed)\n",
    "    df_X_train, df_X_val, df_y_train, df_y_val = train_test_split(df_X_temp, df_y_temp, test_size=int(val_ratio*N), random_state=seed)\n",
    "    \n",
    "    print(\"Number of training samples:\", df_X_train.shape[0])\n",
    "    print(\"Number of validation samples:\", df_X_val.shape[0])\n",
    "    print(\"Number of test samples:\", df_X_test.shape[0])\n",
    "    print(\"Number of covariates:\", p)\n",
    "        \n",
    "    ''' Processing Covariates '''    \n",
    "    continuous_features = df_X.columns\n",
    "    continuous_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean'))])\n",
    "\n",
    "    x_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('continuous', continuous_transformer, continuous_features)])\n",
    "\n",
    "    X_train = x_preprocessor.fit_transform(df_X_train)\n",
    "    X_val = x_preprocessor.transform(df_X_val)\n",
    "    X_test = x_preprocessor.transform(df_X_test)\n",
    "    \n",
    "    x_scaler = StandardScaler()\n",
    "    X_train = x_scaler.fit_transform(X_train)\n",
    "    X_val = x_scaler.transform(X_val)\n",
    "    X_test = x_scaler.transform(X_test)    \n",
    "    X_train = np.round(X_train, decimals=6)\n",
    "    X_val = np.round(X_val, decimals=6)\n",
    "    X_test = np.round(X_test, decimals=6)\n",
    "    \n",
    "    ''' Processing Target Responses '''\n",
    "    if standardize_response:\n",
    "        y_preprocessor = StandardScaler()\n",
    "    else:\n",
    "        def identity_func(x):\n",
    "            return np.array(x)\n",
    "        y_preprocessor = FunctionTransformer(lambda x: np.array(x)) # acts as identity\n",
    "\n",
    "    y_train = y_preprocessor.fit_transform(df_y_train)\n",
    "    y_val = y_preprocessor.transform(df_y_val)\n",
    "    y_test = y_preprocessor.transform(df_y_test)\n",
    "                \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, (x_preprocessor, x_scaler), y_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, df_y, _ = load_data(\n",
    "                                  filename='pdb2019trv3_us.csv',\n",
    "                                  remove_margin_of_error_variables=True)\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "X, Y, Xval, Yval, Xtest, Ytest, _, y_scaler = process_data(\n",
    "    df_X,\n",
    "    df_y,\n",
    "    val_ratio=0.01, \n",
    "    test_ratio=0.2,\n",
    "    seed=seed,\n",
    "    standardize_response=True)\n",
    "\n",
    "colnames = df_X.columns\n",
    "xTrain = pd.DataFrame(X,columns = df_X.columns)\n",
    "xTest = pd.DataFrame(Xtest,columns = df_X.columns)\n",
    "yTrain = np.ravel(Y.copy())\n",
    "yTest = np.ravel(Ytest.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc14afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing as mp\n",
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdcc2bf",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f418217",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "xTrain_sub = xTrain.copy()\n",
    "xTest_sub = xTest.copy()\n",
    "\n",
    "rf = RandomForestRegressor(max_depth = 6, n_estimators = 500, max_features = 'sqrt', n_jobs = -1).fit(xTrain_sub,yTrain)\n",
    "pred = rf.predict(xTest_sub)\n",
    "print(mean_squared_error(yTest,pred))\n",
    "\n",
    "yTest_org = np.ndarray.flatten(y_scaler.inverse_transform(yTest.reshape(-1,1)))\n",
    "pred_org = np.ndarray.flatten(y_scaler.inverse_transform(pred.reshape(-1,1)))\n",
    "print(np.sqrt(mean_squared_error(yTest_org,pred_org)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab802072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree1 = DecisionTreeRegressor(max_depth = 4).fit(xTrain,yTrain)\n",
    "pred = tree1.predict(xTest_sub)\n",
    "print(mean_squared_error(yTest,pred))\n",
    "\n",
    "yTest_org = np.ndarray.flatten(y_scaler.inverse_transform(yTest.reshape(-1,1)))\n",
    "pred_org = np.ndarray.flatten(y_scaler.inverse_transform(pred.reshape(-1,1)))\n",
    "print(np.sqrt(mean_squared_error(yTest_org,pred_org)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f340b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(np.column_stack((rf.feature_importances_, xTrain_sub.columns)),\n",
    "             columns = ['imp','feats'])\n",
    "feature_importance.sort_values('imp', ascending = False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_small = RandomForestRegressor(max_depth = 2, n_estimators = 10, max_features = 'sqrt'\n",
    "                                 , n_jobs = -1).fit(xTrain_sub,yTrain)\n",
    "\n",
    "pred = rf_small.predict(xTest_sub)\n",
    "print(mean_squared_error(yTest,pred))\n",
    "\n",
    "yTest_org = np.ndarray.flatten(y_scaler.inverse_transform(yTest.reshape(-1,1)))\n",
    "pred_org = np.ndarray.flatten(y_scaler.inverse_transform(pred.reshape(-1,1)))\n",
    "print(np.sqrt(mean_squared_error(yTest_org,pred_org)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbd11a",
   "metadata": {},
   "source": [
    "### Single Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a463fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list = np.array(rf.estimators_)\n",
    "tree1 = tree_list[0]\n",
    "\n",
    "import sys\n",
    "from EnsemblePlot.EnsemblePlot import EnsemblePlot\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "L1 = np.arange(len(xTrain_sub.columns))\n",
    "##############################################################\n",
    "L2 = []\n",
    "for s in xTrain.columns:\n",
    "    s = s.replace('_CEN_2010', '')\n",
    "    s = s.replace('_ACS_13_17','')\n",
    "    s = s.replace('_','-')\n",
    "    s = s.replace('pct','Pct')\n",
    "    L2.append(s)\n",
    "##############################################################\n",
    "feature_names = {k:v for k,v in zip(L1,L2)}\n",
    "feature_names[-2] = 'leaf'\n",
    "ep = EnsemblePlot([],[],[],[])\n",
    "\n",
    "G_all, graphs = ep.prune_ensemble_graph([tree1], [[1,1,1,1,1,1]],feature_names, method = 'layers')\n",
    "\n",
    "\n",
    "node_colors,cmap, legend_array = ep.get_colors(G_all, plot_legend = False)\n",
    "\n",
    "mapping = legend_array[0]\n",
    "\n",
    "\n",
    "scalarMap = legend_array[1]\n",
    "pos1 = ep.pos_grid_layout(G_all, 2)\n",
    "\n",
    "fig = plt.figure(figsize = (18,14))\n",
    "\n",
    "n1 = nx.draw_networkx_nodes(G_all,pos1, node_color = node_colors, cmap = cmap,\n",
    "                            node_size = 800, edgecolors = 'black' )\n",
    "e1 = nx.draw_networkx_edges(G_all,pos1)\n",
    "for label in mapping:\n",
    "    plt.plot([0],[0],color=scalarMap.to_rgba(mapping[label]),label=label,linewidth = 10)\n",
    "plt.legend( prop={'size': 18},handlelength = .5,bbox_to_anchor=(1, 1.05))\n",
    "\n",
    "plt.savefig(path+'/census_single_tree.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca436039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features_used:', sum(tree1.feature_importances_>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0bcf7a",
   "metadata": {},
   "source": [
    "### Forest Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list = np.array(rf.estimators_)\n",
    "W_array = nodes_per_layer(tree_list)\n",
    "normalization = total_nodes(tree_list)\n",
    "\n",
    "learning_rate = 1/len(tree_list)\n",
    "\n",
    "base_err = sklearn.metrics.mean_squared_error(yTest,rf.predict(xTest_sub))\n",
    "\n",
    "base_rf_nodes = 0\n",
    "for tree1 in tree_list:\n",
    "    base_rf_nodes = base_rf_nodes + tree1.tree_.node_count\n",
    "\n",
    "\n",
    "diff_array_list = difference_array_list(xTrain_sub,tree_list)\n",
    "diff_test_array_list = difference_array_list(xTest_sub,tree_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4be52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(seed)\n",
    "best_alpha = 15\n",
    "vars_best,_ = solve_weighted(pd.Series(yTrain),tree_list,diff_array_list,best_alpha,learning_rate,\n",
    "                            W_array,normalization,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1c3a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "coef_best = prune_polish_l0(diff_array_list,yTrain,vars_best,learning_rate,K, time_limit = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_err, pred_polish = evaluate_test_error_polished(diff_test_array_list,yTest,vars_best,\n",
    "                                                              coef_best,learning_rate)\n",
    "print(pruned_err)\n",
    "\n",
    "pred_polish_org = np.ndarray.flatten(y_scaler.inverse_transform(pred_polish.reshape(-1,1)))\n",
    "print(np.sqrt(mean_squared_error(yTest_org,pred_polish_org)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from EnsemblePlot.EnsemblePlot import EnsemblePlot\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "L1 = np.arange(len(xTrain_sub.columns))\n",
    "##############################################################\n",
    "L2 = []\n",
    "for s in xTrain.columns:\n",
    "    s = s.replace('_CEN_2010', '')\n",
    "    s = s.replace('_ACS_13_17','')\n",
    "    s = s.replace('_','-')\n",
    "    s = s.replace('pct','Pct')\n",
    "    L2.append(s)\n",
    "##############################################################\n",
    "feature_names = {k:v for k,v in zip(L1,L2)}\n",
    "feature_names[-2] = 'leaf'\n",
    "ep = EnsemblePlot([],[],[],[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trees_ordered = tree_list[[sum(i) != 0 for i in vars_best]][np.argsort(-np.abs(coef_best))]\n",
    "vars_ordered = vars_best[[sum(i) != 0 for i in vars_best]][np.argsort(-np.abs(coef_best))]\n",
    "coefs = -np.sort(-np.abs(coef_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f80b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "new_rc_params = {'text.usetex': True,\n",
    "         'svg.fonttype': 'none',\n",
    "         'font.size': 16,\n",
    "         'font.family': \"Times Roman\",\n",
    "         'mathtext.fontset': 'custom',\n",
    "         'mathtext.rm': 'Times Roman',\n",
    "         'mathtext.it': 'Times Roman:italic',\n",
    "         'mathtext.bf': 'Times Roman:bold'\n",
    "         }\n",
    "matplotlib.rcParams.update(new_rc_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_all, graphs = ep.prune_ensemble_graph(trees_ordered[:K], vars_ordered[:K],feature_names, method = 'layers')\n",
    "\n",
    "\n",
    "node_colors,cmap, legend_array = ep.get_colors(G_all, plot_legend = False)\n",
    "\n",
    "mapping = legend_array[0]\n",
    "\n",
    "\n",
    "scalarMap = legend_array[1]\n",
    "pos1 = ep.pos_grid_layout(G_all, 2)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = plt.subplot(111)\n",
    "nx.draw(G_all,pos1,node_color = node_colors, \n",
    "        cmap = cmap,node_size = 400 , edgecolors = 'black' ,  with_labels = False)\n",
    "for label in mapping:\n",
    "    plt.plot([0],[0],color=scalarMap.to_rgba(mapping[label]),label=label,linewidth = 10)\n",
    "    \n",
    "#plt.legend( prop={'size': 12})\n",
    "ax.legend(bbox_to_anchor=(.9, 1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "n1 = nx.draw_networkx_nodes(G_all,pos1, node_color = node_colors, cmap = cmap,\n",
    "                            node_size = 400, edgecolors = 'black' )\n",
    "e1 = nx.draw_networkx_edges(G_all,pos1)\n",
    "for label in mapping:\n",
    "    plt.plot([0],[0],color=scalarMap.to_rgba(mapping[label]),label=label,linewidth = 10)\n",
    "plt.legend( prop={'size': 12})\n",
    "ax.legend(bbox_to_anchor=(1.25, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03341554",
   "metadata": {},
   "source": [
    "### Analyze Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "new_rc_params = {'text.usetex': False,\n",
    "         'svg.fonttype': 'none',\n",
    "         'font.size': 40,\n",
    "         'font.family': \"Times Roman\",\n",
    "         'mathtext.fontset': 'custom',\n",
    "         'mathtext.rm': 'Times Roman',\n",
    "         'mathtext.it': 'Times Roman:italic',\n",
    "         'mathtext.bf': 'Times Roman:bold'\n",
    "         }\n",
    "matplotlib.rcParams.update(new_rc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91473c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 7\n",
    "\n",
    "tree1 = trees_ordered[ind]\n",
    "vars1 = vars_ordered[ind]\n",
    "G_all, graphs = ep.prune_ensemble_graph([tree1], [vars1],feature_names, method = 'layers')\n",
    "print(coefs[ind])\n",
    "\n",
    "node_colors,cmap, legend_array = ep.get_colors(G_all, plot_legend = False)\n",
    "\n",
    "mapping = legend_array[0]\n",
    "\n",
    "\n",
    "scalarMap = legend_array[1]\n",
    "pos1 = ep.pos_grid_layout(G_all, 2)\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = plt.subplot(111)\n",
    "nx.draw(G_all,pos1,node_color = node_colors, \n",
    "        cmap = cmap,node_size = 400 , edgecolors = 'black' ,  with_labels = False)\n",
    "for label in mapping:\n",
    "    plt.plot([0],[0],color=scalarMap.to_rgba(mapping[label]),label=label,linewidth = 10)\n",
    "    \n",
    "#plt.legend( prop={'size': 12})\n",
    "ax.legend(bbox_to_anchor=(.9, 1.))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (32,12))\n",
    "a = sklearn.tree.plot_tree(tree1,max_depth = sum(vars1),feature_names = xTrain.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff56f3",
   "metadata": {},
   "source": [
    "### Additive Model From Ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56f84d28",
   "metadata": {},
   "source": [
    "124.32*(0.354)*I(Pct-Renter-Occp_HU <= 0.275) \n",
    "+ 124.32*(-0.698)*I(Pct-Renter-Occp-HU > 0.275) \n",
    "\n",
    "+ 102.17*(-0.549)*I(Children-in-Pov <= 0.025)*I(Pct-NH-White-alone <= -0.363)\n",
    "+ 108.75*(0.548)*I(Children-in-Pov <= 0.025)*I(Pct-NH-White-alone > -0.363)\n",
    "+ 108.75*(-1.547)*I(Children-in-Pov > 0.025)*I(Pct-NH-White-alone <= -0.733)\n",
    "+ 108.75*(-0.06)*I(Children-in-Pov > 0.025)*I(Pct-NH-White-alone > -0.733)\n",
    "\n",
    "+ 102.17*(0.512)*I(Pct-ENG-VW-SPAN <= 0.287)*I(Pct-Female-No-HB <= 0.239)\n",
    "+ 102.17*(-0.808)*I(Pct-ENG-VW-SPAN <= 0.287)*I(Pct-Female-No-HB > 0.239)\n",
    "+ 102.17*(-1.581)*I(Pct-ENG-VW-SPAN > 0.287)*I(Pct-NH-White-alone <= -1.111)\n",
    "+ 102.17*(-0.504)*I(Pct-ENG-VW-SPAN > 0.287)*I(Pct-NH-White-alone > -1.111)\n",
    "\n",
    "+ 95.01*(0.546)*I(Pct-NH-Blk-alone <= 0.0)*I(Pct-Not-HS-Grad <= 0.29)\n",
    "+ 95.01*(-0.82)*I(Pct-NH-Blk-alone <= 0.0)*I(Pct-Not-HS-Grad > 0.29)\n",
    "+ 95.01*(-1.97)*I(Pct-NH-Blk-alone > 0.0)*I(Pct-HHD-w-Computer <= -0.362)\n",
    "+ 95.01*(-0.36)*I(Pct-NH-Blk-alone > 0.0)*I(Pct-HHD-w-Computer > -0.362)\n",
    "\n",
    "+ 93.36*(-0.671)*I(Pct-College <= -0.642) \n",
    "+ 93.36*(0.319)*I(Pct-College > -0.642) \n",
    "\n",
    "+ 92.35*(0.455)*I(Pct-No-Health-Ins <= -0.047) \n",
    "+ 92.35*(-0.612)*I(Pct-No-Health-Ins > -0.047) \n",
    "\n",
    "+ 91.96*(-1.233)*I(Pct-NH-White-alone <= -0.901) \n",
    "+ 91.96*(0.314)*I(Pct-NH-White-alone > -0.901)\n",
    "\n",
    "+ 91.60*(0.575)*I(Pct-Pop-18-24 <= -0.192) \n",
    "+ 91.60*(-0.539)*I(Pct-Pop-18-24 > -0.192)\n",
    "\n",
    "+ 89.51*(0.473)*I(Hispanic <= 0.293)*I(Pct-NH-Blk-alone <= 0.556)\n",
    "+ 89.51*(-0.989)*I(Hispanic <= 0.293)*I(Pct-NH-Blk-alone > 0.556)\n",
    "+ 89.51*(-0.436)*I(Hispanic > 0.293)*I(Pct-Not-HS-Grad <= 1.159)\n",
    "+ 89.51*(-1.593)*I(Hispanic > 0.293)*I(Pct-Not-HS-Grad > 1.159)\n",
    "\n",
    "+ 71.60*(0.353)*I(Pct-HHD-w-OnlySPhne <= 0.198) \n",
    "+ 71.60**(-0.699)*I(Pct-HHD-w-OnlySPhne  > 0.198) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3bc629e",
   "metadata": {},
   "source": [
    "predicted_response_rate = \n",
    "\n",
    "44.00*I(Pct-Renter-Occp-HU <= 0.275) \n",
    "- 86.8*I(Pct-Renter-Occp-HU > 0.275) \n",
    "\n",
    "- 56.09*(-0.549)*I(Children-in-Pov <= 0.025)*I(Pct-NH-White-alone <= -0.363)\n",
    "+ 59.60*I(Children-in-Pov <= 0.025)*I(Pct-NH-White-alone > -0.363)\n",
    "- 168.24*(-1.547)*I(Children-in-Pov > 0.025)*I(Pct-NH-White-alone <= -0.733)\n",
    "- 6.52*I(Children-in-Pov > 0.025)*I(Pct-NH-White-alone > -0.733)\n",
    "\n",
    "\n",
    "+ 52.31*I(Pct-ENG-VW-SPAN <= 0.287)*I(Pct-Female-No-HB <= 0.239)\n",
    "- 82.55*I(Pct-ENG-VW-SPAN <= 0.287)*I(Pct-Female-No-HB > 0.239)\n",
    "- 161.53*I(Pct-ENG-VW-SPAN > 0.287)*I(Pct-NH-White-alone <= -1.111)\n",
    "- 51.49*I(Pct-ENG-VW-SPAN > 0.287)*I(Pct-NH-White-alone > -1.111)\n",
    "\n",
    "+ 51.87*I(Pct-NH-Blk-alone <= 0.0)*I(Pct-Not-HS-Grad <= 0.29)\n",
    "- 77.91*I(Pct-NH-Blk-alone <= 0.0)*I(Pct-Not-HS-Grad > 0.29)\n",
    "- 187.17*I(Pct-NH-Blk-alone > 0.0)*I(Pct-HHD-w-Computer <= -0.362)\n",
    "- 34.20*I(Pct-NH-Blk-alone > 0.0)*I(Pct-HHD-w-Computer > -0.362)\n",
    "\n",
    "- 62.64*I(Pct-College <= -0.642) \n",
    "+ 29.78*I(Pct-College > -0.642) \n",
    "\n",
    "+ 42.02*I(Pct-No-Health-Ins <= -0.047) \n",
    "- 56.52*I(Pct-No-Health-Ins > -0.047) \n",
    "\n",
    "- 113.39*I(Pct-NH-White-alone <= -0.901) \n",
    "+ 28.88*I(Pct-NH-White-alone > -0.901)\n",
    "\n",
    "+ 52.67*I(Pct-Pop-18-24 <= -0.192) \n",
    "- 49.37*I(Pct-Pop-18-24 > -0.192)\n",
    "\n",
    "+ 42.24*I(Hispanic <= 0.293)*I(Pct-NH-Blk-alone <= 0.556)\n",
    "- 88.53*I(Hispanic <= 0.293)*I(Pct-NH-Blk-alone > 0.556)\n",
    "- 39.026*I(Hispanic > 0.293)*I(Pct-Not-HS-Grad <= 1.159)\n",
    "- 142.59*I(Hispanic > 0.293)*I(Pct-Not-HS-Grad > 1.159)\n",
    "\n",
    "+ 25.27*I(Pct-HHD-w-OnlySPhne <= 0.198) \n",
    "- 50.05*I(Pct-HHD-w-OnlySPhne  > 0.198) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e16023",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrees = 6\n",
    "G_all_full, graphs_full = ep.prune_ensemble_graph(trees_ordered[:ntrees], \n",
    "                        [np.ones(6) for i in range(ntrees)] ,feature_names, method = 'layers')\n",
    "\n",
    "node_colors1,cmap1, legend_array = ep.get_colors(G_all_full, plot_legend = False)\n",
    "mapping1 = legend_array[0]\n",
    "\n",
    "pos2  = ep.pos_grid_layout(G_all_full, 3)\n",
    "\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "n1 = nx.draw_networkx_nodes(G_all_full,pos2,  node_color = node_colors1, cmap = cmap1,\n",
    "                            node_size = 90, edgecolors = 'black' , linewidths = .15)\n",
    "\n",
    "e1 = nx.draw_networkx_edges(G_all_full,pos2, width = 0.5, arrowsize = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
