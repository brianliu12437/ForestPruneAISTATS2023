{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import time\n",
    "from numba import jit\n",
    "import itertools\n",
    "import random   \n",
    "import time\n",
    "import warnings\n",
    "import gc\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import lasso_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Functions\n",
    "def load_openml(data_id,ordinal = True, y_label = ''):\n",
    "    \"\"\"Load dataset by id from OpenML. If ordinal == True, encode categorical columns \n",
    "    via ordinal encoding. If ordinal == False then encode categorical columns with dummy vars.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset1 = sklearn.datasets.fetch_openml(data_id = data_id,as_frame = True)\n",
    "    name = dataset1.details['name']\n",
    "    X, y = dataset1.data, dataset1.target \n",
    "    data = pd.DataFrame(X,columns = dataset1.feature_names)\n",
    "\n",
    "    if len(y_label) == 0:\n",
    "        data['y'] = y\n",
    "    else:\n",
    "        data['y'] = y[y_label]\n",
    "\n",
    "    #shuffle index\n",
    "    data = data.sample(frac = 1)\n",
    "    y = data['y']\n",
    "    y = y.astype(float)\n",
    "    X = data.drop('y',axis = 1)\n",
    "\n",
    "    #encode categorical columns\n",
    "    cat = list(set(X.columns) - set(X.select_dtypes(include=np.number).columns.tolist()))\n",
    "    \n",
    "    if ordinal == True:\n",
    "        for col in cat:\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "            X[col] = X[col].fillna(max(X[col]+1))\n",
    "        \n",
    "    elif ordinal == False:\n",
    "        X = pd.get_dummies(X,columns = cat)\n",
    "    \n",
    "        \n",
    "    #impute median\n",
    "#    X = X.fillna(X.median()) \n",
    "    return X,y,name\n",
    "\n",
    "def get_node_depths(tree1):\n",
    "    \"\"\"\n",
    "    Get the node depths of the decision tree\n",
    "\n",
    "    >>> d = DecisionTreeClassifier()\n",
    "    >>> d.fit([[1,2,3],[4,5,6],[7,8,9]], [1,2,3])\n",
    "    >>> get_node_depths(d.tree_)\n",
    "    array([0, 1, 1, 2, 2])\n",
    "    \"\"\"\n",
    "    def get_node_depths_(current_node, current_depth, l, r, depths):\n",
    "        depths += [current_depth]\n",
    "        if l[current_node] != -1 and r[current_node] != -1:\n",
    "            get_node_depths_(l[current_node], current_depth + 1, l, r, depths)\n",
    "            get_node_depths_(r[current_node], current_depth + 1, l, r, depths)\n",
    "\n",
    "    depths = []\n",
    "    get_node_depths_(0, 0, tree1.tree_.children_left, tree1.tree_.children_right, depths) \n",
    "    return np.array(depths)\n",
    "\n",
    "def get_node_count(tree_list,best_vars):\n",
    "    num_nodes = 0\n",
    "    depths = np.sum(best_vars,axis = 1)\n",
    "    for i in range(len(best_vars)):\n",
    "        tree1 = tree_list[i]\n",
    "        node_depths = get_node_depths(tree1)\n",
    "        depth_cutoff = depths[i]\n",
    "        if depth_cutoff > 0:\n",
    "            num_nodes = num_nodes + sum(node_depths <= depth_cutoff)\n",
    "    return num_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd316ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_array_list(X,tree_list):\n",
    "    diff_array_list = []\n",
    "    for tree1 in tree_list:\n",
    "        diff_array_list.append(difference_array(X,tree1))\n",
    "    return np.array(diff_array_list)\n",
    "\n",
    "def difference_array(X, tree_learner):\n",
    "    \"\"\"function that takes a decision tree and returns an \n",
    "    [m,d]\n",
    "    Each row is an instance and each column is a depth level.\n",
    "    We take the difference in internal node values to get the delta for each depth level.\n",
    "    the column sum of the output is the prediction of the tree\n",
    "    \"\"\"\n",
    "    \n",
    "    node_indicator = tree_learner.decision_path(X)\n",
    "    values = tree_learner.tree_.value\n",
    "    vdiffs = []\n",
    "    \n",
    "    for i in range(0,len(X)):\n",
    "        node_ids = node_indicator.indices[node_indicator.indptr[i] : node_indicator.indptr[i + 1]]\n",
    "        instance_values = np.ndarray.flatten(values[node_ids])\n",
    "        diffs = [j-i for i, j in zip(instance_values[:-1], instance_values[1:])]\n",
    "        row = np.zeros(tree_learner.max_depth)\n",
    "        row[:len(diffs)] = diffs\n",
    "        vdiffs.append(row)\n",
    "        \n",
    "    return np.array(vdiffs)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluate_test_error(difference_array_list,Y,vars_z,learning_rate):\n",
    "    pred = np.zeros(len(Y))\n",
    "    for i in range(len(vars_z)):\n",
    "        pred += np.dot(difference_array_list[i],vars_z[i])*learning_rate     \n",
    "    return np.square(np.subtract(Y, pred)).mean()\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import itertools\n",
    "import random    \n",
    "@jit(nopython=True)\n",
    "def precompute_predictions(diff_array_list,temp_vars,learning_rate,cycle_ind):\n",
    "    \n",
    "    precompute_pred = np.zeros(len(diff_array_list[0]))    \n",
    "    for i in range(len(diff_array_list)):\n",
    "        if i != cycle_ind:\n",
    "            precompute_pred += np.dot(diff_array_list[i],temp_vars[i])*learning_rate \n",
    "   \n",
    "    return precompute_pred\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluate_candidates(diff_array_list,temp_vars,learning_rate,cycle_ind,candidates,\n",
    "                        precompute_pred,Y,alpha,W_array, normalization):\n",
    "    scores = []\n",
    "    for candidate in candidates:\n",
    "        temp_vars[cycle_ind] = candidate\n",
    "        pred_candidate = np.dot(diff_array_list[cycle_ind],candidate)*learning_rate\n",
    "        pred = np.add(precompute_pred,pred_candidate)\n",
    "        err = np.sum((Y-pred)**2)/len(Y) + (alpha/normalization)*np.sum(np.dot(W_array[cycle_ind],candidate))\n",
    "        scores.append(err)\n",
    "    return scores\n",
    "\n",
    "@jit(nopython=True)\n",
    "def eval_obj(Y,diff_array_list,vars_z,learning_rate,alpha,W_array,normalization):\n",
    "    pred = np.zeros(len(Y))\n",
    "    regularization = 0\n",
    "    for i in range(len(vars_z)):\n",
    "        pred+= learning_rate*np.dot(diff_array_list[i],vars_z[i])\n",
    "        regularization += np.sum(np.dot(W_array[i],vars_z[i]))\n",
    "    \n",
    "    bias = np.sum((Y-pred)**2)/len(Y)\n",
    "    \n",
    "    return bias + regularization*alpha/normalization\n",
    "\n",
    "@jit(nopython=True)\n",
    "def converge_test(sequence, threshold,tail_length):\n",
    "    diff = np.diff(sequence)\n",
    "    if len(diff) < (tail_length+1):\n",
    "        return False\n",
    "    else:\n",
    "        return (np.max(np.abs(diff[-tail_length:])) < threshold)\n",
    "\n",
    "\n",
    "def solve_weighted(Y,tree_list,diff_array_list,alpha,learning_rate,\n",
    "                                          W_array,normalization,warm_start= []):\n",
    "    max_depth = tree_list[0].max_depth\n",
    "    Y = np.array(Y.values)\n",
    "    \n",
    "    vars_z = np.zeros((len(tree_list),max_depth))\n",
    "    if len(warm_start) > 0:\n",
    "        vars_z = np.array(warm_start)\n",
    "    \n",
    "    candidates = np.vstack([np.zeros(max_depth),np.tril(np.ones((max_depth,max_depth)))])\n",
    "    \n",
    "    convergence_scores = np.array([])\n",
    "    converged = False\n",
    "    ind_counter = 0\n",
    "    local_best = 9999\n",
    "    total_inds = 0\n",
    "    while converged == False:\n",
    "        \n",
    "        cycle_ind = ind_counter % len(vars_z)   \n",
    "\n",
    "        temp_vars= vars_z.copy()\n",
    "        precompute_pred = precompute_predictions(diff_array_list,temp_vars,learning_rate,cycle_ind)\n",
    "        scores = evaluate_candidates(diff_array_list,temp_vars,learning_rate,cycle_ind,\n",
    "                                     candidates,precompute_pred,Y,alpha,W_array,normalization)\n",
    "        \n",
    "        vars_z[cycle_ind] = candidates[np.argmin(scores)]\n",
    "        convergence_scores = np.append(convergence_scores,eval_obj(Y,diff_array_list,\n",
    "                                                                   vars_z,learning_rate,alpha,W_array,normalization))\n",
    "        converged = converge_test(np.array(convergence_scores),10**-6,3)\n",
    "        \n",
    "        ind_counter = ind_counter + 1\n",
    "        total_inds = total_inds + 1\n",
    "        \n",
    "        #local search\n",
    "        if converged == True:\n",
    "            support_indicies = np.where(~np.all(vars_z == 0, axis=1))[0]\n",
    "            zero_indicies = np.where(np.all(vars_z == 0, axis=1))[0]\n",
    "            \n",
    "            if convergence_scores[-1] > local_best:\n",
    "                converged = True\n",
    "            \n",
    "            elif len(support_indicies)> 0:\n",
    "                local_ind = random.choice(support_indicies)\n",
    "                vars_z[local_ind] = np.zeros(max_depth)\n",
    "                \n",
    "                if len(zero_indicies) > 0:\n",
    "                    ind_counter = min(zero_indicies)\n",
    "                    converged = False\n",
    "                    local_best = convergence_scores[-1]\n",
    "                \n",
    "                else:\n",
    "                    converged = True\n",
    "     \n",
    "    return vars_z , total_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d57588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Penalties\n",
    "\n",
    "def nodes_per_layer(tree_list):\n",
    "    max_depth = tree_list[0].max_depth\n",
    "    results = []\n",
    "    for tree1 in tree_list:\n",
    "        depths = get_node_depths(tree1)\n",
    "        values,counts = np.unique(depths,return_counts = True)\n",
    "        diag = np.zeros(max_depth)\n",
    "        counts = counts[1:]\n",
    "        diag[:len(counts)] = counts\n",
    "        results.append(np.diag(diag))\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "def total_nodes(tree_list):\n",
    "    return np.sum(tree1.tree_.node_count for tree1 in tree_list) - len(tree_list)\n",
    "\n",
    "def prune_polish(difference_array_list,Y,vars_z,learning_rate):\n",
    "    pred_array = []\n",
    "    for i in range(len(vars_z)):\n",
    "        if sum(vars_z[i])>0:\n",
    "            pred_array.append(np.dot(difference_array_list[i],vars_z[i])*learning_rate)\n",
    "    \n",
    "    if len(pred_array) == 0:\n",
    "        return np.zeros(len(vars_z))\n",
    "    \n",
    "    pred_array = np.transpose(pred_array)\n",
    "    lm = Ridge(alpha = 0.01, fit_intercept = False).fit(pred_array,Y)\n",
    "    coef = lm.coef_\n",
    "    return coef\n",
    "\n",
    "@jit(nopython=True)\n",
    "def evaluate_test_error_polished(difference_array_list,Y,vars_z,coef,learning_rate):\n",
    "    pred = np.zeros(len(Y))\n",
    "    j = 0\n",
    "    for i in range(len(vars_z)):\n",
    "        if sum(vars_z[i])>0:\n",
    "            pred += np.dot(difference_array_list[i],vars_z[i])*learning_rate*coef[j]  \n",
    "            j+=1\n",
    "    return np.square(np.subtract(Y, pred)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaed706",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa9ce84a",
   "metadata": {},
   "source": [
    "215\t2dplanes\n",
    "344\tmv\n",
    "564\tfried\n",
    "574\thouse_16H\n",
    "537\thouses\n",
    "216\televators\n",
    "189\tkin8nm\n",
    "227\tcpu_small\n",
    "308\tpuma32H\n",
    "558\tbank32nh\n",
    "503\twind\n",
    "287\twine_quality\n",
    "405\tmtp\n",
    "183\tabalone\n",
    "507\tspace_ga\n",
    "550\tquake\n",
    "512\tballoon\n",
    "41021\tMoneyball\n",
    "541\tsocmob\n",
    "223\tstock\n",
    "549\tstrikes *\n",
    "\n",
    "531\tboston *\n",
    "547\tno2 *\n",
    "196\tautoMpg *\n",
    "505\ttecator\n",
    "\n",
    "528\thumandevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4298aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def subensemble_predict(X,tree_list,learning_rate,ntrees):\n",
    "    pred = np.zeros(len(X))\n",
    "    for tree1 in tree_list[:ntrees]:\n",
    "        pred += tree1.predict(X)*learning_rate\n",
    "    return pred\n",
    "\n",
    "def get_node_count_all(tree_list):\n",
    "    num_nodes = 0\n",
    "    for tree1 in tree_list:\n",
    "        num_nodes = num_nodes + tree1.tree_.node_count\n",
    "    return num_nodes\n",
    "\n",
    "def lasso_predict(X,tree_list,coef):\n",
    "    pred = np.zeros(len(X))\n",
    "    for i in range(len(tree_list)):\n",
    "        pred += tree_list[i].predict(X)*coef[i]\n",
    "    return pred\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from itertools import product\n",
    "\n",
    "def l0_ensemble_select(features, response,node_count, node_limit, warm_up=None, verbose=False, time_limit = 600):\n",
    "    \"\"\"\n",
    "    Deploy and optimize the MIQP formulation of L0-Regression.\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    assert isinstance(node_limit, (int, np.integer))\n",
    "    regressor = gp.Model()\n",
    "    samples, dim = features.shape\n",
    "    assert samples == response.shape[0]\n",
    "\n",
    "\n",
    "    # Append a column of ones to the feature matrix to account for the y-intercept\n",
    "    X = np.concatenate([features, np.ones((samples, 1))], axis=1)  \n",
    "    \n",
    "    # Decision variables\n",
    "    beta = regressor.addVars(dim, lb=-GRB.INFINITY, name=\"beta\") # Weights\n",
    " \n",
    "    # iszero[i] = 1 if beta[i] = 0  \n",
    "    iszero = regressor.addVars(dim, vtype=GRB.BINARY, name=\"iszero\") \n",
    "    \n",
    "    # Objective Function (OF): minimize 1/2 * RSS using the fact that\n",
    "    # if x* is a minimizer of f(x), it is also a minimizer of k*f(x) iff k > 0\n",
    "    Quad = np.dot(X.T, X)\n",
    "    lin = np.dot(response.T, X)\n",
    "    obj = sum(0.5 * Quad[i,j] * beta[i] * beta[j]\n",
    "              for i, j in product(range(dim), repeat=2))\n",
    "    obj -= sum(lin[i] * beta[i] for i in range(dim))\n",
    "    obj += 0.5 * np.dot(response, response)\n",
    "    regressor.setObjective(obj, GRB.MINIMIZE)\n",
    "    \n",
    "    # Constraint sets\n",
    "    for i in range(dim):\n",
    "        # If iszero[i]=1, then beta[i] = 0\n",
    "        regressor.addSOS(GRB.SOS_TYPE1, [beta[i], iszero[i]])\n",
    "        \n",
    "        \n",
    "    regressor.addConstr(sum([node_count[i]*(1-iszero[i]) \\\n",
    "                             for i in range(len(node_count))]) <= node_limit) # Budget constraint\n",
    "    \n",
    "    # We may use the Lasso or prev solution with fewer features as warm start\n",
    "    if warm_up is not None and len(warm_up) == dim:\n",
    "        for i in range(dim):\n",
    "            iszero[i].start = (abs(warm_up[i]) < 1e-6)    \n",
    "    if not verbose:\n",
    "        regressor.params.OutputFlag = 0\n",
    "    regressor.params.timelimit = time_limit\n",
    "    regressor.params.mipgap = 0.001\n",
    "  \n",
    "    regressor.optimize()\n",
    "\n",
    "    coeff = np.array([beta[i].X for i in range(dim)])\n",
    "    t2 = time.time()\n",
    "    return  coeff, (t2 - t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee56a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = [528,\n",
    "505,\n",
    "196,\n",
    "547,\n",
    "531,\n",
    "223,\n",
    "541,\n",
    "41021,\n",
    "512,\n",
    "507,\n",
    "183,\n",
    "42570,\n",
    "405\n",
    ",287\n",
    ",503\n",
    ",189\n",
    ",227\n",
    ",308\n",
    ",558\n",
    ",201,\n",
    "216,\n",
    "537,\n",
    "574]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc29f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "max_depth = 20\n",
    "n_estimators = 100\n",
    "ntree_range = list(range(1,n_estimators,10))\n",
    "\n",
    "reproduce = []\n",
    "baseline = []\n",
    "for i in inds:\n",
    "    X,y,name = load_openml(i,False)\n",
    "    y = pd.Series(y)\n",
    "    y.index = X.index\n",
    "\n",
    "\n",
    "\n",
    "    learning_rate = 1/n_estimators\n",
    "    results_all = pd.DataFrame()\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "\n",
    "    threshold1 = 0.05\n",
    "    depths_all = []\n",
    "    \n",
    "    print(name)\n",
    "    k = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        k = k+1\n",
    "        xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "        yTrain, yTest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        xTrain, xVal, yTrain, yVal = train_test_split(xTrain, yTrain, test_size=0.25) \n",
    "\n",
    "        xTrain = preprocessing.StandardScaler().fit_transform(xTrain)\n",
    "        xTrain = pd.DataFrame(xTrain,columns = X.columns)\n",
    "        xVal = preprocessing.StandardScaler().fit_transform(xVal)\n",
    "        xVal = pd.DataFrame(xVal,columns = X.columns)\n",
    "        xTest = preprocessing.StandardScaler().fit_transform(xTest)\n",
    "        xTest = pd.DataFrame(xTest,columns = X.columns)\n",
    "        yTrain = preprocessing.StandardScaler().fit_transform(yTrain.values.reshape(-1, 1))\n",
    "        yTest = preprocessing.StandardScaler().fit_transform(yTest.values.reshape(-1, 1))\n",
    "        yVal = preprocessing.StandardScaler().fit_transform(yVal.values.reshape(-1, 1))\n",
    "\n",
    "        yTrain = pd.Series(yTrain.flatten())\n",
    "        yTrain.index = xTrain.index\n",
    "        yTest = pd.Series(yTest.flatten())\n",
    "        yTest.index = xTest.index\n",
    "        yVal = pd.Series(yVal.flatten())\n",
    "        yVal.index = xVal.index\n",
    "\n",
    "\n",
    "        model = RandomForestRegressor(max_depth = max_depth,n_estimators = n_estimators,\n",
    "                                     max_features = 'sqrt', n_jobs = 4).fit(xTrain,yTrain)\n",
    "\n",
    "        tree_list = np.array(model.estimators_)\n",
    "        W_array = nodes_per_layer(tree_list)\n",
    "        normalization = total_nodes(tree_list)\n",
    "        \n",
    "        base_err = sklearn.metrics.mean_squared_error(yTest,model.predict(xTest))\n",
    "        val_err = sklearn.metrics.mean_squared_error(yVal,model.predict(xVal))\n",
    "        \n",
    "        base_rf_nodes = 0\n",
    "        for tree1 in tree_list:\n",
    "            base_rf_nodes = base_rf_nodes + tree1.tree_.node_count\n",
    "\n",
    "\n",
    "        diff_array_list = difference_array_list(xTrain,tree_list)\n",
    "        diff_test_array_list = difference_array_list(xTest,tree_list)\n",
    "        diff_val_array_list = difference_array_list(xVal,tree_list)\n",
    "\n",
    "        alphas = []\n",
    "        warm_start = []\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for alpha in np.flip(np.logspace(-2,1.5,50)):\n",
    "            \n",
    "            vars1,iters = solve_weighted(yTrain,tree_list,diff_array_list,\n",
    "                                    alpha,learning_rate,W_array,normalization, warm_start = warm_start)\n",
    "            warm_start = vars1\n",
    "            coef = prune_polish(diff_array_list,yTrain,vars1,learning_rate)\n",
    "            err = evaluate_test_error_polished(diff_val_array_list,yVal.values,vars1,\n",
    "                                                          coef,learning_rate)\n",
    "            results.append([alpha,err,iters])\n",
    "\n",
    "        results_df = pd.DataFrame(results,columns = ['alpha', 'err','iter'])\n",
    "\n",
    "\n",
    "        list_errs = []\n",
    "        list_nodes = []\n",
    "        list_threshold = []\n",
    "        \n",
    "        for i in [0.01,0.025,0.05]:\n",
    "            UL = val_err + i\n",
    "            best_alpha = np.max(results_df[results_df['err']<UL]['alpha'])\n",
    "            if np.isnan(best_alpha):\n",
    "                best_alpha = results_df['alpha'].min()\n",
    "            print(best_alpha)\n",
    "            \n",
    "            \n",
    "            vars_best,_ = solve_weighted(yTrain,tree_list,diff_array_list,best_alpha,learning_rate,\n",
    "                                        W_array,normalization,)\n",
    "\n",
    "            coef_best = prune_polish(diff_array_list,yTrain,vars_best,learning_rate)\n",
    "\n",
    "            pruned_err = evaluate_test_error_polished(diff_test_array_list,yTest.values,vars_best,\n",
    "                                                              coef_best,learning_rate)\n",
    "            pruned_rf_nodes = get_node_count(tree_list,vars_best)\n",
    "            nodes_limit = pruned_rf_nodes\n",
    "            prune_score = pruned_err\n",
    "            \n",
    "            \n",
    "            # baseline\n",
    "            base_results = []\n",
    "            for ntrees in ntree_range:\n",
    "                pred_test = subensemble_predict(xVal,tree_list,learning_rate,ntrees)\n",
    "                score = mean_squared_error(yVal,pred_test)\n",
    "                nnodes = get_node_count_all(tree_list[:ntrees])\n",
    "                base_results.append([ntrees,score,nnodes])\n",
    "                \n",
    "            base_results = pd.DataFrame(base_results,columns = ['ntrees','score','nnodes'])\n",
    "            best_trees = base_results[base_results['nnodes']<=nodes_limit]['ntrees'].max()\n",
    "            \n",
    "            if not math.isnan(best_trees):\n",
    "                pred_test = subensemble_predict(xTest,tree_list,learning_rate,best_trees)\n",
    "            \n",
    "            else:\n",
    "                pred_test = np.repeat(yTrain.mean(),len(yTest))\n",
    "            \n",
    "            base_score = mean_squared_error(yTest,pred_test)\n",
    "            \n",
    "            #lasso\n",
    "            pred_array = []\n",
    "            for tree1 in tree_list:\n",
    "                pred_array.append(tree1.predict(xTrain))\n",
    "            pred_array = np.transpose(pred_array)\n",
    "            alphas, coef_path, _ = lasso_path(pred_array, yTrain ,n_alphas = 100, eps = 10**-10)\n",
    "            coef_path = np.transpose(coef_path) \n",
    "\n",
    "            lasso_results = []\n",
    "            for coef in coef_path:\n",
    "                subforest = np.array(tree_list)[coef > 0]\n",
    "                lasso_pred = lasso_predict(xVal,subforest,coef[coef>0])\n",
    "                lasso_score = mean_squared_error(yVal,lasso_pred)\n",
    "                lasso_nnodes = get_node_count_all(subforest)\n",
    "                lasso_ntrees = sum(coef>0)\n",
    "                lasso_results.append([lasso_ntrees,lasso_score,lasso_nnodes])\n",
    "                \n",
    "                \n",
    "            lasso_results = pd.DataFrame(lasso_results,columns = ['ntrees','score','nnodes'])\n",
    "            \n",
    "            if sum(lasso_results['nnodes'] <= nodes_limit) != 0:\n",
    "                best_coef = coef_path[lasso_results[lasso_results['score']\\\n",
    "                  == lasso_results[lasso_results['nnodes']<=nodes_limit]['score'].min()].index[0]]\n",
    "                best_subforest = np.array(tree_list)[best_coef > 0]\n",
    "                lasso_pred = lasso_predict(xTest,best_subforest,best_coef[best_coef>0])\n",
    "                lasso_score = mean_squared_error(yTest, lasso_pred)\n",
    "            else:\n",
    "                best_coef = np.zeros(len(tree_list))\n",
    "                lasso_score = 1.0\n",
    "            \n",
    "            ##### L0\n",
    "            node_count = np.array([tree1.tree_.node_count for tree1 in tree_list])\n",
    "            pred_test = np.transpose([tree1.predict(xTest) for tree1 in tree_list])\n",
    "            \n",
    "            \n",
    "            l0_coef,l0_time = l0_ensemble_select(pred_array,\n",
    "                                                 yTrain,node_count,node_limit = nodes_limit,\n",
    "                                                  warm_up = best_coef ,\n",
    "                                                 time_limit = 60)\n",
    "            l0_score = mean_squared_error(yTest,pred_test@l0_coef)\n",
    "            \n",
    "            ##### Store Results\n",
    "            \n",
    "            baseline.append([name,k,i,prune_score,base_score,lasso_score,l0_score,l0_time])\n",
    "            reproduce.append([name,k,i,nodes_limit,train_index, test_index ])\n",
    "            \n",
    "            print([name,k,i,prune_score,base_score,lasso_score,l0_score,l0_time])\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
